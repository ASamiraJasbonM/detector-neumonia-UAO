#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
M√≥dulo para carga y gesti√≥n del modelo de red neuronal convolucional
Modelo principal: 'conv_MLP_84.h5'
"""

import tensorflow as tf
from tensorflow.keras.models import load_model
import os
import numpy as np

# ‚úÖ CORREGIDO: Configuraci√≥n de TensorFlow en un solo lugar
tf.compat.v1.disable_eager_execution()
tf.compat.v1.experimental.output_all_intermediates(True)

def model_fun():
    """
    Funci√≥n principal para cargar el modelo pre-entrenado.
    Busca el modelo en varias ubicaciones posibles.
    
    Returns:
        tf.keras.Model: Modelo cargado listo para predicci√≥n o None en caso de error
    """
    try:
        # ‚úÖ CORREGIDO: Rutas relativas a tu estructura de proyecto
        possible_paths = [
            'models/conv_MLP_84.h5',           # Desde ra√≠z
            '../models/conv_MLP_84.h5',        # Desde src/modulos
            '../../models/conv_MLP_84.h5',     # Desde otras ubicaciones
        ]
        
        model_path = None
        for path in possible_paths:
            if os.path.exists(path):
                model_path = path
                break
        
        if model_path is None:
            raise FileNotFoundError("No se encontr√≥ el modelo en ninguna ubicaci√≥n posible")
        
        print(f"üîÑ Cargando modelo desde: {model_path}")
        
        # Cargar modelo sin compilar inicialmente
        model = load_model(model_path, compile=False)
        
        # ‚úÖ MEJORADO: Compilar con configuraci√≥n optimizada
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # Validar que el modelo est√© listo
        if validar_modelo_cargado(model):
            print(f"‚úÖ Modelo cargado exitosamente: {model_path}")
            print(f"   - Capas: {len(model.layers)}")
            print(f"   - Par√°metros: {model.count_params():,}")
            return model
        else:
            print("‚ùå El modelo cargado no pas√≥ la validaci√≥n")
            return None
        
    except Exception as e:
        print(f"‚ùå Error cr√≠tico cargando el modelo: {e}")
        return None

def validar_modelo_cargado(model):
    """
    Valida que el modelo cargado tenga la estructura esperada.
    
    Args:
        model (tf.keras.Model): Modelo a validar
        
    Returns:
        bool: True si el modelo es v√°lido, False en caso contrario
    """
    if model is None:
        return False
    
    try:
        # Verificar que tenga la capa necesaria para Grad-CAM
        layer_names = [layer.name for layer in model.layers]
        
        if 'conv10_thisone' not in layer_names:
            print("‚ö†Ô∏è  No se encontr√≥ la capa 'conv10_thisone' para Grad-CAM")
            print(f"   Capas disponibles: {[name for name in layer_names if 'conv' in name]}")
            # No retornar False, solo advertir
        
        # Probar una predicci√≥n de prueba
        test_input = np.random.rand(1, 512, 512, 1).astype(np.float32)
        test_output = model.predict(test_input, verbose=0)
        
        if test_output.shape[1] == 3:  # Debe tener 3 clases
            print("‚úÖ Modelo validado: arquitectura correcta")
            return True
        else:
            print(f"‚ùå Modelo tiene {test_output.shape[1]} clases, se esperaban 3")
            return False
            
    except Exception as e:
        print(f"‚ùå Error validando modelo: {e}")
        return False

# ‚úÖ MANTENIDO: Funciones adicionales para futuras extensiones
def load_custom_model(model_path):
    """Carga un modelo desde una ruta espec√≠fica"""
    try:
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Archivo no encontrado: {model_path}")
        
        model = load_model(model_path, compile=False)
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        
        print(f"‚úÖ Modelo personalizado cargado: {model_path}")
        return model
        
    except Exception as e:
        print(f"‚ùå Error cargando modelo personalizado: {e}")
        return None